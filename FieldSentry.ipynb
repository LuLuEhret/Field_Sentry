{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FieldSentry.py is a script that checks the status of the sensors, weather and screens of the installations\n",
    "from insolAPI.WebAPI import API\n",
    "import simplejson as json\n",
    "import pandas as pd\n",
    "import pendulum as pdl\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import requests\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SHOW_PLOT = False\n",
    "\n",
    "\n",
    "list_sensor = [\n",
    "    \"PAR\",\n",
    "    \"IRRAD\",\n",
    "    \"GII\",\n",
    "    \"DNI\",\n",
    "    \"DHI\",\n",
    "    \"TEMP\",\n",
    "    \"HUMI\",\n",
    "    \"RAIN\",\n",
    "    \"RAIN_TYPE\",\n",
    "    \"RAIN_ACCUMULATED\",\n",
    "    \"WIND\",\n",
    "    \"WIND_DIR\",\n",
    "    \"VIRTUAL\",\n",
    "    \"LEAF_TEMP\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "dict_screen_mode = {\n",
    "    0: \"\",\n",
    "    1: \"Auto\",\n",
    "    2: \"Manual\",\n",
    "    3: \"Emergency\",\n",
    "    4: \"Protection\",\n",
    "    5: \"Demo\",\n",
    "    6: \"Remote\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def read_json_config():\n",
    "    \"\"\"\n",
    "    Read the config.json file and return the api key\n",
    "    Parameters and response: https://openweathermap.org/forecast5\n",
    "    \"\"\"\n",
    "    with open(\"config/config.json\") as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    installations = {}\n",
    "\n",
    "    # Iterate through locations\n",
    "    locations = config_data['locations']\n",
    "    for location in locations:\n",
    "        # Store details in a dictionary\n",
    "        location_details = {\n",
    "            'id': location['id'],\n",
    "            'name': location['name'],\n",
    "            'latitude': location['latitude'],\n",
    "            'longitude': location['longitude'],\n",
    "            'wind_threshold': location['wind_threshold'],\n",
    "            'high_temperature_threshold': location['high_temperature_threshold'],\n",
    "            'has_a_screen': location['has_a_screen'],\n",
    "        }\n",
    "        installations[location['name']] = location_details\n",
    "    return installations, [config_data['api_key'], config_data['api_url']]\n",
    "\n",
    "\n",
    "\n",
    "def format_timestamp(original_timestamp_str):\n",
    "    original_timestamp = datetime.datetime.strptime(original_timestamp_str, '%Y-%m-%d %H:%M:%S')\n",
    "    formatted_timestamp_str = original_timestamp.strftime('%Hh %d-%m-%Y')\n",
    "    return formatted_timestamp_str\n",
    "\n",
    "\n",
    "\n",
    "def format_timestamps_in_dict(input_dict):\n",
    "    formatted_dict = {}\n",
    "    for key, sub_dict in input_dict.items():\n",
    "        formatted_dict[key] = {}\n",
    "        for event, timestamps in sub_dict.items():\n",
    "            formatted_timestamps = [format_timestamp(ts) for ts in timestamps]\n",
    "            formatted_dict[key][event] = formatted_timestamps\n",
    "    return formatted_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_weather_forecast(dict_instal, api_data, city_name):\n",
    "    \"\"\"\n",
    "    Request the weather forecast for the next 2 days, every 3 hours\n",
    "    \"\"\"\n",
    "    params = {\"appid\": api_data[0], \"cnt\": \"20\", \"units\": \"metric\"}\n",
    "\n",
    "    params[\"lat\"] = dict_instal[city_name][\"latitude\"]\n",
    "    params[\"lon\"] = dict_instal[city_name][\"longitude\"]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(api_data[1], params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        update_request_count()\n",
    "        return data\n",
    "\n",
    "    except Exception as ex:\n",
    "        # print(f\" \\n Error: {ex}\")\n",
    "        #generate an empty dict to avoid errors\n",
    "        error_dict = {'list': [{'dt': 0,\n",
    "                    'main': {'temp': 0,\n",
    "                        'feels_like': 0,\n",
    "                        'temp_min': 0,\n",
    "                        'temp_max': 0,\n",
    "                        'pressure': 0,\n",
    "                        'sea_level': 0,\n",
    "                        'grnd_level': 0,\n",
    "                        'humidity': 0,\n",
    "                        'temp_kf': 0},\n",
    "                    'weather': [{'id': 0,\n",
    "                        'main': 'Rain',\n",
    "                        'description': 'light rain',\n",
    "                        'icon': '10d'}],\n",
    "                    'clouds': {'all': 0},\n",
    "                    'wind': {'speed': 0, 'deg': 0, 'gust': 0},\n",
    "                    'visibility': 0,\n",
    "                    'pop': 0,\n",
    "                    'rain': {'3h': 0},\n",
    "                    'sys': {'pod': 'd'},\n",
    "                    'dt_txt': '9999-01-01 00:00:00'}]}\n",
    "        return error_dict  # noqa: E501\n",
    "\n",
    "\n",
    "\n",
    "def plot_weather_forecast(weather_data, city_name):\n",
    "    \"\"\"\n",
    "    If called, plot the weather forecast for the next 2 days\n",
    "    \"\"\"\n",
    "    forecast_date = []\n",
    "    forecast_temp = []\n",
    "    forecast_snow = []\n",
    "    forecast_rain = []\n",
    "    forecast_wind = []\n",
    "    forecast_pop = []\n",
    "\n",
    "    for forecast in weather_data[\"list\"]:\n",
    "        forecast_date.append(forecast[\"dt_txt\"])\n",
    "        forecast_temp.append(forecast[\"main\"][\"temp\"])\n",
    "        forecast_wind.append(forecast[\"wind\"][\"speed\"])\n",
    "        forecast_pop.append(forecast[\"pop\"])\n",
    "        try:\n",
    "            forecast_snow.append(forecast[\"snow\"][\"3h\"])\n",
    "        except:\n",
    "            forecast_snow.append(0)\n",
    "        try:\n",
    "            forecast_rain.append(forecast[\"rain\"][\"3h\"])\n",
    "        except:\n",
    "            forecast_rain.append(0)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecast_date,\n",
    "            y=forecast_temp,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Temperature\",\n",
    "            line=dict(color=\"orange\", width=2),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecast_date,\n",
    "            y=forecast_snow,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Snow\",\n",
    "            line=dict(color=\"lightblue\", width=2),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecast_date,\n",
    "            y=forecast_rain,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Rain\",\n",
    "            line=dict(color=\"darkcyan\", width=2),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecast_date,\n",
    "            y=forecast_wind,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Wind\",\n",
    "            line=dict(color=\"darkred\", width=2),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=forecast_date,\n",
    "            y=forecast_pop,\n",
    "            name=\"Precipitation Probability\",\n",
    "            marker_color=\"lightgrey\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(\n",
    "        title=\"Weather Forecast for \" + city_name + \"\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Values\",\n",
    "        legend=dict(x=1, y=1, traceorder=\"normal\"),\n",
    "        # Background color of the entire graph area,\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor=\"rgba(0,0,0,0.1)\")\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor=\"rgba(0,0,0,0.1)\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "def alert_user(weather_data, dict_events):\n",
    "    \"\"\"\n",
    "    Set the threshold for the alerts, and which alerts to send\n",
    "    \"\"\"\n",
    "    alert_list = []\n",
    "    for forecast in weather_data[\"list\"]:\n",
    "        try:\n",
    "            if bool(forecast[\"snow\"][\"3h\"]):\n",
    "                alert_list.append(\"Snow fall\")\n",
    "                alert_list.append(forecast[\"dt_txt\"])\n",
    "        except:\n",
    "            pass\n",
    "        if forecast[\"wind\"][\"speed\"] > dict_events[\"wind_threshold\"]:\n",
    "            alert_list.append(\"Strong wind\")\n",
    "            alert_list.append(forecast[\"dt_txt\"])\n",
    "        if forecast[\"main\"][\"temp\"] > dict_events[\"high_temperature_threshold\"]:\n",
    "            alert_list.append(\"High temperature\")\n",
    "            alert_list.append(forecast[\"dt_txt\"])\n",
    "        #to add a new alert, add the condition here. Also need to change the main\n",
    "    # if forecast[\"dt\"] == 0:\n",
    "    #     print(\"\\nError: no weather data available\")\n",
    "    return alert_list\n",
    "\n",
    "\n",
    "\n",
    "def update_request_count():\n",
    "    \"\"\"\n",
    "    update the number of requests made to the API, and the number of requests made during the last hour\n",
    "    \"\"\"\n",
    "    file_name = \"reports/count_requests.csv\"\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_name):\n",
    "        # Read the last line and get the count\n",
    "        with open(file_name, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines:\n",
    "                last_line = lines[-1].strip()\n",
    "                count = int(last_line.split(\",\")[1]) + 1\n",
    "            else:\n",
    "                count = 1\n",
    "            # add the number of requests made during the last hour\n",
    "            last_hour = datetime.datetime.now() - datetime.timedelta(hours=1)\n",
    "            count_last_hour = 1\n",
    "            for line in lines:\n",
    "                if (\n",
    "                    datetime.datetime.strptime(\n",
    "                        line.split(\",\")[4].strip(), \"%Y-%m-%d %H:%M:%S\"\n",
    "                    )\n",
    "                    >= last_hour\n",
    "                ):\n",
    "                    count_last_hour = count_last_hour + 1\n",
    "    else:\n",
    "        # If the file doesn't exist, create it and set count to 1\n",
    "        count = 1\n",
    "        count_last_hour = 1\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    with open(file_name, \"a\") as file:\n",
    "        file.write(f\"Total,{count},last hour,{count_last_hour},{timestamp}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def save_alerts_to_csv(df):\n",
    "    \"\"\"\n",
    "    update the csv file with the alerts\n",
    "    \"\"\"\n",
    "    df['Timestamp'] = datetime.datetime.now().strftime('%H:%M:%S %d-%m-%Y')\n",
    "\n",
    "    #replace \\n with a \",\"\n",
    "    df = df.replace('\\n', ', ', regex=True)\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    df = df[[cols[-1]] + cols[:-1]]\n",
    "\n",
    "\n",
    "    #check if the file exists\n",
    "    if not os.path.exists(\"reports/log_reports.csv\"):\n",
    "        df.to_csv(\"reports/log_reports.csv\", index=False, header=True) # add the header if the file doesn't exist\n",
    "    else:\n",
    "        df.to_csv(\"reports/log_reports.csv\", mode='a', header=False, index=False)\n",
    "        with open(\"reports/log_reports.csv\", \"a\") as file:\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def list_to_string(lst):\n",
    "    \"\"\"\n",
    "    Convert a list to a string, to be able to print it\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return \"\\n\".join(map(str, lst))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def process_screen_data(df):\n",
    "    \"\"\"\n",
    "    logics to process the screen data\n",
    "    \"\"\"\n",
    "    df = df[['screen_id', 'state', 'name']]\n",
    "    screen_names = df['name'].unique()\n",
    "    list_states = []\n",
    "    for screen_name in screen_names:\n",
    "        df_screen_tmp = df[df['name'] == screen_name]\n",
    "        df_screen_tmp = df.sort_index(ascending=False)\n",
    "        state = df_screen_tmp['state'][0]\n",
    "        if state != 1:\n",
    "            list_states.append(f\"{screen_name}: {dict_screen_mode[state]}\")\n",
    "    if screen_names.size == 0:\n",
    "        list_states.append(\"No logs for 2d+\")\n",
    "    return list_states\n",
    "\n",
    "\n",
    "\n",
    "def print_progress_bar(percentage, length=10):\n",
    "    # print(percentage)\n",
    "    if np.isnan(percentage):\n",
    "        percentage = 1\n",
    "    block = int(round(length * percentage))\n",
    "    progress = \"[\" + \"#\" * block + \"-\" * (length - block) + \"]\"\n",
    "    # print(f\"\\r{progress}\", end=\"\", flush=True)\n",
    "    return progress\n",
    "\n",
    "\n",
    "def last_logs(dict_instal, list_sensor, api):\n",
    "    time_args = dict(\n",
    "        start=pdl.yesterday().subtract(weeks=1).to_datetime_string(),\n",
    "        stop=(pdl.now()).to_datetime_string(),\n",
    "        timezone = timezone('Europe/Zurich')\n",
    "    )\n",
    "\n",
    "    logs_joined = {}\n",
    "    dict_list_theoretical = {}\n",
    "    unique_sensors = {}\n",
    "    logs_joined_unique = {}\n",
    "    last_log = {}\n",
    "    time_diff = {}\n",
    "    print(\"\\nCollecting data...\\n\")\n",
    "    for instal in tqdm(dict_instal):\n",
    "        logs_joined[instal] = {}\n",
    "        dict_list_theoretical[instal] = []\n",
    "        sensor_number = 0\n",
    "        for sensor_type in api.SensorsTypes:\n",
    "            if str(sensor_type).split(\".\")[1] in list_sensor:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                    logs_joined[instal][list_sensor[sensor_number]] = api.get_sensor_channels_logs_joined(**time_args,sensor_type=sensor_type, install=dict_instal[instal][\"id\"])\n",
    "                    sensor_number += 1\n",
    "                    try:\n",
    "                        theoretical_sensor = api.get_sensor_channels(sensor_type=sensor_type, install=dict_instal[instal][\"id\"])\n",
    "                        theoretical_sensor = theoretical_sensor[theoretical_sensor[\"deleted_at\"].isna()].sensor_name.unique()\n",
    "                        dict_list_theoretical[instal].extend(theoretical_sensor)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "    print(\"\\nProcessing data...\\n\")\n",
    "    for instal in dict_instal:\n",
    "        unique_sensors[instal] = []\n",
    "        time_diff[instal] = {}\n",
    "        for sensor_type in logs_joined[instal]:\n",
    "            try:\n",
    "                unique_sensors[instal].extend(logs_joined[instal][sensor_type][\"sensor_name\"].unique())\n",
    "            except :\n",
    "                pass\n",
    "        unique_sensors[instal] = list(set(unique_sensors[instal]))\n",
    "        logs_joined_unique[instal] = {}\n",
    "        for sensor_type in logs_joined[instal]:\n",
    "            for unique_sensor in unique_sensors[instal]:\n",
    "                try:\n",
    "                    if unique_sensor in logs_joined[instal][sensor_type][\"sensor_name\"].unique():\n",
    "                        logs_joined_unique[instal][unique_sensor] = logs_joined[instal][sensor_type].loc[logs_joined[instal][sensor_type][\"sensor_name\"] == unique_sensor]\n",
    "                        logs_joined_unique[instal][unique_sensor].index = logs_joined_unique[instal][unique_sensor].index.round('min')\n",
    "                        logs_joined_unique[instal][unique_sensor] = logs_joined_unique[instal][unique_sensor].loc[~logs_joined_unique[instal][unique_sensor].index.duplicated(keep='first')]\n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "\n",
    "        for sensor in logs_joined_unique[instal]:\n",
    "            logs_joined_unique[instal][sensor] = logs_joined_unique[instal][sensor].dropna(subset=[logs_joined_unique[instal][sensor].columns[1]])\n",
    "            time_difference = logs_joined_unique[instal][sensor].index.to_series().diff()\n",
    "            time_difference = time_difference[time_difference > pd.Timedelta(minutes=2)].sum()\n",
    "            time_diff[instal][sensor] = time_difference\n",
    "\n",
    "        last_log[instal] = {}\n",
    "        for sensor in dict_list_theoretical[instal]:\n",
    "            try:\n",
    "                last_log[instal][sensor] = logs_joined_unique[instal][sensor].index[-1]\n",
    "            except:\n",
    "                last_log[instal][sensor] = \"> 1 week\"\n",
    "            try:\n",
    "                if last_log[instal][sensor] > pdl.now().subtract(minutes=10):\n",
    "                    last_log[instal][sensor] = \"Online\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    #make a df with the last log and the time difference for each sensor\n",
    "    dict_df = {}\n",
    "    for instal in last_log:\n",
    "        dict_df[instal] = pd.DataFrame.from_dict(last_log[instal], orient=\"index\", columns=[\"Last log\"])\n",
    "        dict_df[instal][\"Time offline (1w)\"] = dict_df[instal].index.map(time_diff[instal])\n",
    "        dict_df[instal][\"% offline\"] = dict_df[instal][\"Time offline (1w)\"].apply(lambda x: print_progress_bar(x.total_seconds() / (7 * 24 * 60 * 60)))\n",
    "\n",
    "    # #sort the sensors by the last log\n",
    "    # for instal in last_log:\n",
    "    #     last_log[instal] = {k: v for k, v in sorted(last_log[instal].items(), key=lambda item: item[1])}\n",
    "\n",
    "\n",
    "    for instal, df in dict_df.items():\n",
    "        # Sort the DataFrame by the \"Last log\" column\n",
    "        df_sorted = df.sort_values(by=[\"Last log\", \"Time offline (1w)\"], ascending=[True, False])\n",
    "\n",
    "        print(f\"{instal}\")\n",
    "        table = tabulate(df_sorted, headers=\"keys\", tablefmt=\"psql\", showindex=True)\n",
    "        print(table)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    saving = input(\"Save table to excel file? (y/n)\")\n",
    "    if saving == \"y\":\n",
    "        writer = pd.ExcelWriter('reports/last_log.xlsx', engine='openpyxl')\n",
    "        for instal, df in dict_df.items():\n",
    "            df.to_excel(writer, sheet_name=instal)\n",
    "        writer.close()\n",
    "        print(\"File saved as 'last_log.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "## Collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to the API\n",
      "Collecting data...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:16<00:00,  4.22s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"config/api_credits_path.json\") as f:\n",
    "        installation_path = json.load(f)[\"path\"]\n",
    "\n",
    "    # installation_path = \"C:/Users/Insolight/Desktop/InsolReports/Installations/\"\n",
    "    with open(installation_path + \"local.json\") as f:\n",
    "        local_data = json.load(f)\n",
    "\n",
    "    api = API(local_data[\"API_user\"], local_data[\"API_pwd\"], dev_space=False)\n",
    "    api.get_sensor_channels(sensor_type=api.SensorsTypes.TEMP, install=23)\n",
    "    print(\"✅ Successfully connected to the API\\nCollecting data...\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ {e}\")\n",
    "    print(\"\\nExiting...\")\n",
    "    sys.stdout.flush()\n",
    "    sys.exit(0)\n",
    "\n",
    "time_args = dict(\n",
    "start=pdl.now().subtract(days=0, hours=0, minutes=30).to_datetime_string(),\n",
    "stop=pdl.now().subtract(days=0, hours=0).to_datetime_string(),\n",
    "timezone=timezone('Europe/Zurich'),\n",
    ")\n",
    "time_args_screens = dict(\n",
    "    start=pdl.now().subtract(days=2, hours=2, minutes=30).to_datetime_string(),\n",
    "    stop=pdl.now().subtract(days=0, hours=2).to_datetime_string(),\n",
    "    timezone=timezone('Europe/Zurich'),\n",
    ")\n",
    "\n",
    "# declarations of the dictionaries\n",
    "dict_instal_json, api_data = read_json_config()\n",
    "dict_instal_logs = {}  # type: dict\n",
    "dict_sensor_channel_id = {}  # type: dict\n",
    "diff_logs = {}  # type: dict\n",
    "dict_logs_joined = {}  # type: dict\n",
    "dict_channel_id = {}  # type: dict\n",
    "dict_missing_sensors = {}  # type: dict\n",
    "dict_weather_data = {} # type: dict\n",
    "dict_alerts = {} # type: dict\n",
    "dict_time_of_snow = {} # type: dict\n",
    "dict_time_of_wind = {} # type: dict\n",
    "dict_time_high_T = {} # type: dict\n",
    "dict_alert_time = {} # type: dict\n",
    "dict_screen_states = {} # type: dict\n",
    "\n",
    "#main loop\n",
    "for instal in tqdm(dict_instal_json):\n",
    "    list_channel_id = [] # type: list\n",
    "    list_sensor_logging = [] # type: list\n",
    "    dict_logs_joined[instal] = {}\n",
    "    dict_sensor_channel_id[instal] = {}\n",
    "    sensor_number = 0\n",
    "    dict_alert_time[instal] = {\"Snow fall\": [], \"Strong wind\": [], \"High temperature\": []}\n",
    "    list_snow_time = []\n",
    "    list_wind_time = []\n",
    "    list_highT_time = []\n",
    "    dict_df_screen = {}\n",
    "    list_screen_states = []\n",
    "\n",
    "\n",
    "    # get the weather forecast for each installation\n",
    "    dict_weather_data[instal] = get_weather_forecast(dict_instal_json, api_data, instal)\n",
    "    if SHOW_PLOT:\n",
    "        plot_weather_forecast(dict_weather_data[instal], instal)\n",
    "    dict_alerts[instal] = alert_user(dict_weather_data[instal], dict_instal_json[instal])\n",
    "\n",
    "    for i in range(0, len(dict_alerts[instal]), 2):\n",
    "        if dict_alerts[instal][i] == \"Snow fall\":\n",
    "            list_snow_time.append(dict_alerts[instal][i + 1])\n",
    "            dict_alert_time[instal][\"Snow fall\"].append(dict_alerts[instal][i + 1])\n",
    "        if dict_alerts[instal][i] == \"Strong wind\":\n",
    "            list_wind_time.append(dict_alerts[instal][i + 1])\n",
    "            dict_alert_time[instal][\"Strong wind\"].append(dict_alerts[instal][i + 1])\n",
    "        if dict_alerts[instal][i] == \"High temperature\":\n",
    "            list_highT_time.append(dict_alerts[instal][i + 1])\n",
    "            dict_alert_time[instal][\"High temperature\"] = dict_alerts[instal][i + 1]\n",
    "\n",
    "\n",
    "    #screen data\n",
    "    if dict_instal_json[instal][\"has_a_screen\"]:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            dict_df_screen[instal] = api.get_screens_logs_joined(**time_args_screens, install=dict_instal_json[instal][\"id\"])\n",
    "            list_screen_states = process_screen_data(dict_df_screen[instal])\n",
    "    dict_screen_states[instal] = list_screen_states\n",
    "\n",
    "\n",
    "    # get all the sensors and channels for each installation\n",
    "    for sensor_type in api.SensorsTypes:\n",
    "        if str(sensor_type).split(\".\")[1] in list_sensor:\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "                    dict_logs_joined[instal][\n",
    "                        list_sensor[sensor_number]\n",
    "                    ] = api.get_sensor_channels_logs_joined(\n",
    "                        **time_args,\n",
    "                        sensor_type=sensor_type,\n",
    "                        install=dict_instal_json[instal][\"id\"],\n",
    "                    )\n",
    "                    list_sensor_logging.extend(\n",
    "                        dict_logs_joined[instal][\n",
    "                            list_sensor[sensor_number]\n",
    "                        ].sensor_channel_id.unique()\n",
    "                    )\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "                    dict_sensor_channel_id[instal][\n",
    "                        list_sensor[sensor_number]\n",
    "                    ] = api.get_sensor_channels(\n",
    "                        sensor_type=sensor_type, install=dict_instal_json[instal][\"id\"]\n",
    "                    )\n",
    "\n",
    "                    list_channel_id.extend(\n",
    "                        dict_sensor_channel_id[instal][list_sensor[sensor_number]][dict_sensor_channel_id[instal][list_sensor[sensor_number]][\"deleted_at\"].isna()].index.tolist()\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            sensor_number += 1\n",
    "\n",
    "    dict_instal_logs[instal] = list_sensor_logging  # dict with the list of sensors logging for each installation\n",
    "    dict_channel_id[instal] = list_channel_id       # dict with the list of channels for each installation\n",
    "\n",
    "\n",
    "    # substraction, result is the list of sensors that are not logging, and not deleted from the config\n",
    "    diff_logs[instal] = list(\n",
    "        set(dict_channel_id[instal]) - set(dict_instal_logs[instal])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presenting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 349.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-----------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Installation      | Sensor ID   | Sensor Name                             | Snow fall   | Strong wind    | High temp   | Screen mode     |\n",
      "+===================+=============+=========================================+=============+================+=============+=================+\n",
      "| Agroscope Series  | 130         | leaf_temp_FS1                           |             |                |             | No logs for 2d+ |\n",
      "|                   | 116         | rain                                    |             |                |             |                 |\n",
      "+-------------------+-------------+-----------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Agroscope Berries | 48          | TRH_bottom_Insolagrin03_T60_Portrait_WW |             |                |             | No logs for 2d+ |\n",
      "+-------------------+-------------+-----------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Bioschmid         |             |                                         |             | 09h 22-12-2023 |             | No logs for 2d+ |\n",
      "|                   |             |                                         |             | 12h 22-12-2023 |             |                 |\n",
      "|                   |             |                                         |             | 15h 22-12-2023 |             |                 |\n",
      "|                   |             |                                         |             | 18h 22-12-2023 |             |                 |\n",
      "|                   |             |                                         |             | 21h 22-12-2023 |             |                 |\n",
      "|                   |             |                                         |             | 00h 23-12-2023 |             |                 |\n",
      "+-------------------+-------------+-----------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Etchelecu         |             |                                         |             |                |             |                 |\n",
      "+-------------------+-------------+-----------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "\n",
      "Collecting data...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:41<00:00, 10.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data...\n",
      "\n",
      "Agroscope Series\n",
      "+-----------------------------------+------------+---------------------+\n",
      "|                                   | Last log   | Time offline (1w)   |\n",
      "|-----------------------------------+------------+---------------------|\n",
      "| rain                              | > 1 week   | NaT                 |\n",
      "| leaf_temp_FS1                     | > 1 week   | NaT                 |\n",
      "| TRH_top_Nursery                   | Online     | 0 days 05:37:00     |\n",
      "| TRH01_GT1_tunnel                  | Online     | 0 days 05:07:00     |\n",
      "| TRH02_GT_tunnel                   | Online     | 0 days 04:49:00     |\n",
      "| TRH01_GT_tunnel                   | Online     | 0 days 04:32:00     |\n",
      "| TRH02_GT1_tunnel                  | Online     | 0 days 04:29:00     |\n",
      "| TRH_bottom_Nursery                | Online     | 0 days 04:17:00     |\n",
      "| T_RH1                             | Online     | 0 days 00:58:00     |\n",
      "| TRH01_TX_tunnel                   | Online     | 0 days 00:54:00     |\n",
      "| T_RH_umbrella_bottom              | Online     | 0 days 00:48:00     |\n",
      "| T_RH2                             | Online     | 0 days 00:40:00     |\n",
      "| T_RH_ext_bottom                   | Online     | 0 days 00:22:00     |\n",
      "| GHI_EXT_02                        | Online     | 0 days 00:16:00     |\n",
      "| PAR_EXT_05                        | Online     | 0 days 00:12:00     |\n",
      "| GHI_EXT_04                        | Online     | 0 days 00:10:00     |\n",
      "| PAR_EXT_06                        | Online     | 0 days 00:06:00     |\n",
      "| PAR_EXT_04                        | Online     | 0 days 00:06:00     |\n",
      "| GHI_EXT_03                        | Online     | 0 days 00:04:00     |\n",
      "| PAR01_Nursery                     | Online     | 0 days 00:02:00     |\n",
      "| PAR01_GT1_tunnel                  | Online     | 0 days 00:02:00     |\n",
      "| PAR_umbrella                      | Online     | 0 days 00:02:00     |\n",
      "| PAR_STRAWB                        | Online     | 0 days 00:02:00     |\n",
      "| PAR_ext_bis                       | Online     | 0 days 00:02:00     |\n",
      "| PAR8                              | Online     | 0 days 00:02:00     |\n",
      "| PAR7                              | Online     | 0 days 00:02:00     |\n",
      "| PAR6                              | Online     | 0 days 00:02:00     |\n",
      "| PAR5                              | Online     | 0 days 00:02:00     |\n",
      "| PAR4                              | Online     | 0 days 00:02:00     |\n",
      "| PAR3                              | Online     | 0 days 00:02:00     |\n",
      "| PAR2                              | Online     | 0 days 00:02:00     |\n",
      "| PAR1                              | Online     | 0 days 00:02:00     |\n",
      "| PAR01_GT_tunnel                   | Online     | 0 days 00:00:00     |\n",
      "| PAR01_TX_tunnel                   | Online     | 0 days 00:00:00     |\n",
      "| PAR_umbrella_strawb               | Online     | 0 days 00:00:00     |\n",
      "| GHI_EXT                           | Online     | 0 days 00:00:00     |\n",
      "| weather_station                   | Online     | 0 days 00:00:00     |\n",
      "| T_RH_umbrella_top                 | Online     | 0 days 00:00:00     |\n",
      "| TRH_EXT_top                       | Online     | 0 days 00:00:00     |\n",
      "| wind_speed_ultrasonic             | Online     | 0 days 00:00:00     |\n",
      "| wind_speed                        | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp_Tulameen_E_NurseryTunnel | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp_Tulameen_W_NurseryTunnel | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp_Vajolet_E_NurseryTunnel  | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp_Vajolet_W_NurseryTunnel  | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp_Tulameen_E_Nursery       | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp_Tulameen_W_Nursery       | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp_Vajolet_E_Nursery        | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp_Vajolet_W_Nursery        | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp02_Joly_Insolagrin05_TX   | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp01_Joly_Insolagrin05_TX   | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp02_Flair_Insolagrin05_TX  | Online     | 0 days 00:00:00     |\n",
      "| LeafTemp01_Flair_Insolagrin05_TX  | Online     | 0 days 00:00:00     |\n",
      "| leaf_temp_umbrella1               | Online     | 0 days 00:00:00     |\n",
      "| leaf_temp_insolagrin_1            | Online     | 0 days 00:00:00     |\n",
      "+-----------------------------------+------------+---------------------+\n",
      "\n",
      "\n",
      "Agroscope Berries\n",
      "+------------------------------------------+---------------------------+---------------------+\n",
      "|                                          | Last log                  | Time offline (1w)   |\n",
      "|------------------------------------------+---------------------------+---------------------|\n",
      "| TRH_bottom_Insolagrin03_T60_Portrait_WW  | 2023-12-22 07:30:00+01:00 | 0 days 04:29:00     |\n",
      "| TRH_top_Insolagrin06_T40_Landscape_EW    | Online                    | 5 days 11:54:00     |\n",
      "| PAR01_Insolagrin05_T60_Landscape_EW      | Online                    | 5 days 10:07:00     |\n",
      "| PAR02_Insolagrin04_T40_Portrait_WW       | Online                    | 5 days 10:07:00     |\n",
      "| PAR01_Insolagrin04_T40_Portrait_WW       | Online                    | 5 days 10:07:00     |\n",
      "| PAR01_Insolagrin02_T40_Portrait_EW       | Online                    | 5 days 10:07:00     |\n",
      "| PAR02_Insolagrin02_T40_Portrait_EW       | Online                    | 5 days 10:05:00     |\n",
      "| PAR02_Insolagrin05_T60_Landscape_EW      | Online                    | 5 days 09:47:00     |\n",
      "| PAR04_Insolagrin01_T60_Portrait_EW       | Online                    | 4 days 16:05:00     |\n",
      "| PAR04_Insolagrin03_T60_Portrait_WW       | Online                    | 4 days 13:53:00     |\n",
      "| WIND02_EXT                               | Online                    | 3 days 23:50:00     |\n",
      "| TRH_top_Insolagrin04_T40_Portrait_WW     | Online                    | 0 days 17:53:00     |\n",
      "| TRH_top_Insolagrin02_T40_Portrait_EW     | Online                    | 0 days 14:48:00     |\n",
      "| TRH_top_EXT                              | Online                    | 0 days 06:34:00     |\n",
      "| TRH_bottom_Insolagrin05_T60_Landscape_EW | Online                    | 0 days 05:48:00     |\n",
      "| TRH_top_Insolagrin05_T60_Landscape_EW    | Online                    | 0 days 05:08:00     |\n",
      "| TRH_bottom_Insolagrin06_T40_Landscape_EW | Online                    | 0 days 04:51:00     |\n",
      "| TRH_bottom_Insolagrin04_T40_Portrait_WW  | Online                    | 0 days 04:32:00     |\n",
      "| TRH_bottom_EXT                           | Online                    | 0 days 04:30:00     |\n",
      "| TRH_bottom_Insolagrin01_T60_Portrait_EW  | Online                    | 0 days 04:22:00     |\n",
      "| TRH_bottom_Insolagrin02_T40_Portrait_EW  | Online                    | 0 days 04:13:00     |\n",
      "| TRH_top_Insolagrin01_T60_Portrait_EW     | Online                    | 0 days 03:41:00     |\n",
      "| TRH_top_Insolagrin03_T60_Portrait_WW     | Online                    | 0 days 00:47:00     |\n",
      "| PAR02_EXT                                | Online                    | 0 days 00:41:00     |\n",
      "| PAR01_EXT                                | Online                    | 0 days 00:39:00     |\n",
      "| SOIL_DOWN_Insolagrin02                   | Online                    | 0 days 00:39:00     |\n",
      "| SOIL_UP_Insolagrin02                     | Online                    | 0 days 00:39:00     |\n",
      "| RAIN_EXT                                 | Online                    | 0 days 00:39:00     |\n",
      "| LeafTemp_Z2_side_ext                     | Online                    | 0 days 00:39:00     |\n",
      "| LeafTemp_Z2_side_int                     | Online                    | 0 days 00:39:00     |\n",
      "| LeafTemp_Z2_int_int                      | Online                    | 0 days 00:39:00     |\n",
      "| PAR03_Insolagrin03_T60_Portrait_WW       | Online                    | 0 days 00:19:00     |\n",
      "| PAR02_Insolagrin03_T60_Portrait_WW       | Online                    | 0 days 00:19:00     |\n",
      "| PAR01_Insolagrin06_T40_Landscape_EW      | Online                    | 0 days 00:17:00     |\n",
      "| PAR04_Insolagrin05_T60_Landscape_EW      | Online                    | 0 days 00:17:00     |\n",
      "| PAR03_Insolagrin05_T60_Landscape_EW      | Online                    | 0 days 00:17:00     |\n",
      "| PAR01_Insolagrin03_T60_Portrait_WW       | Online                    | 0 days 00:17:00     |\n",
      "| PAR03_Insolagrin01_T60_Portrait_EW       | Online                    | 0 days 00:17:00     |\n",
      "| PAR02_Insolagrin01_T60_Portrait_EW       | Online                    | 0 days 00:17:00     |\n",
      "| PAR01_Insolagrin01_T60_Portrait_EW       | Online                    | 0 days 00:17:00     |\n",
      "| WIND01_EXT                               | Online                    | 0 days 00:17:00     |\n",
      "| LeafTemp_Z4_int_int                      | Online                    | 0 days 00:17:00     |\n",
      "| PAR04_Insolagrin06_T40_Landscape_EW      | Online                    | 0 days 00:15:00     |\n",
      "| PAR03_Insolagrin06_T40_Landscape_EW      | Online                    | 0 days 00:15:00     |\n",
      "| PAR02_Insolagrin06_T40_Landscape_EW      | Online                    | 0 days 00:15:00     |\n",
      "| PAR04_Insolagrin04_T40_Portrait_WW       | Online                    | 0 days 00:15:00     |\n",
      "| PAR03_Insolagrin04_T40_Portrait_WW       | Online                    | 0 days 00:15:00     |\n",
      "| PAR04_Insolagrin02_T40_Portrait_EW       | Online                    | 0 days 00:15:00     |\n",
      "| PAR03_Insolagrin02_T40_Portrait_EW       | Online                    | 0 days 00:15:00     |\n",
      "+------------------------------------------+---------------------------+---------------------+\n",
      "\n",
      "\n",
      "Bioschmid\n",
      "+------------------+------------+---------------------+\n",
      "|                  | Last log   | Time offline (1w)   |\n",
      "|------------------+------------+---------------------|\n",
      "| WIND02_EXT       | Online     | 5 days 04:42:00     |\n",
      "| PAR01_EXT        | Online     | 4 days 00:42:00     |\n",
      "| TRH02_kontroll   | Online     | 3 days 09:37:00     |\n",
      "| TRH01_kontroll   | Online     | 3 days 04:12:00     |\n",
      "| TRH01_EXT        | Online     | 2 days 16:39:00     |\n",
      "| TRH02_EXT        | Online     | 2 days 03:42:00     |\n",
      "| TRH04_EXT        | Online     | 1 days 07:59:00     |\n",
      "| PAR02_EXT        | Online     | 0 days 21:23:00     |\n",
      "| TRH03_EXT        | Online     | 0 days 19:38:00     |\n",
      "| TRH01_agriverti  | Online     | 0 days 09:31:00     |\n",
      "| TRH01_INSOLAGRIN | Online     | 0 days 07:00:00     |\n",
      "| TRH02_INSOLAGRIN | Online     | 0 days 04:59:00     |\n",
      "| TRH02_agriverti  | Online     | 0 days 04:10:00     |\n",
      "| RAIN01_EXT       | Online     | 0 days 01:42:00     |\n",
      "| GHI_01_EXT       | Online     | 0 days 01:01:00     |\n",
      "| WIND01_EXT       | Online     | 0 days 00:50:00     |\n",
      "| PAR01_INSOLAGRIN | Online     | 0 days 00:44:00     |\n",
      "| PAR02_INSOLAGRIN | Online     | 0 days 00:42:00     |\n",
      "| PAR04_INSOLAGRIN | Online     | 0 days 00:38:00     |\n",
      "| PAR03_INSOLAGRIN | Online     | 0 days 00:38:00     |\n",
      "| PAR02_kontroll   | Online     | 0 days 00:04:00     |\n",
      "| PAR01_kontroll   | Online     | 0 days 00:04:00     |\n",
      "| PAR01_agriverti  | Online     | 0 days 00:02:00     |\n",
      "| PAR03_kontroll   | Online     | 0 days 00:02:00     |\n",
      "| PAR03_agriverti  | Online     | 0 days 00:00:00     |\n",
      "| PAR02_agriverti  | Online     | 0 days 00:00:00     |\n",
      "+------------------+------------+---------------------+\n",
      "\n",
      "\n",
      "Etchelecu\n",
      "+-----------------------+------------+---------------------+\n",
      "|                       | Last log   | Time offline (1w)   |\n",
      "|-----------------------+------------+---------------------|\n",
      "| TRH_top_Insolagrin    | Online     | 3 days 14:29:00     |\n",
      "| TRH_bottom_Insolagrin | Online     | 3 days 14:11:00     |\n",
      "| TRH_bottom_control    | Online     | 0 days 13:51:00     |\n",
      "| TRH_bottom_EXT        | Online     | 0 days 13:24:00     |\n",
      "| PAR01_Insolagrin      | Online     | 0 days 09:04:00     |\n",
      "| PAR02_EXT             | Online     | 0 days 09:02:00     |\n",
      "| PAR01_EXT             | Online     | 0 days 09:02:00     |\n",
      "| PAR04_Insolagrin      | Online     | 0 days 09:02:00     |\n",
      "| PAR03_Insolagrin      | Online     | 0 days 09:02:00     |\n",
      "| PAR02_Insolagrin      | Online     | 0 days 09:02:00     |\n",
      "| TRH_top_EXT           | Online     | 0 days 09:02:00     |\n",
      "| GHI_01_EXT            | Online     | 0 days 09:00:00     |\n",
      "| RAIN_EXT              | Online     | 0 days 09:00:00     |\n",
      "| WIND02_EXT            | Online     | 0 days 09:00:00     |\n",
      "| WIND01_EXT            | Online     | 0 days 09:00:00     |\n",
      "| PAR02_control         | Online     | 0 days 08:59:00     |\n",
      "| PAR01_control         | Online     | 0 days 08:59:00     |\n",
      "| TRH_top_control       | Online     | 0 days 08:59:00     |\n",
      "+-----------------------+------------+---------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for instal in tqdm(dict_instal_json):\n",
    "    #try to identify the missing sensors from the channel list\n",
    "    for sensor_id in diff_logs[instal]:\n",
    "        for sensor in list_sensor:\n",
    "            try:\n",
    "                dict_missing_sensors[sensor_id] = [\n",
    "                    instal,\n",
    "                    dict_sensor_channel_id[instal][sensor]\n",
    "                    .loc[sensor_id]\n",
    "                    .address,\n",
    "                    dict_sensor_channel_id[instal][sensor]\n",
    "                    .loc[sensor_id]\n",
    "                    .sensor_name,\n",
    "                    dict_sensor_channel_id[instal][sensor]\n",
    "                    .loc[sensor_id]\n",
    "                    .channel_name,\n",
    "                ]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "df_missing_sensors = pd.DataFrame.from_dict(\n",
    "    dict_missing_sensors,\n",
    "    orient=\"index\",\n",
    "    columns=[\"installation\", \"address\", \"sensor_name\", \"channel_name\"],\n",
    ")\n",
    "df_missing_sensors = df_missing_sensors.drop_duplicates(subset=[\"sensor_name\"])\n",
    "\n",
    "columns = [\"Installation\", \"Sensor ID\", \"Sensor Name\", \"Snow fall\", \"Strong wind\", \"High temp\", \"Screen mode\"]\n",
    "df_report = pd.DataFrame(columns=columns)\n",
    "\n",
    "dict_alert_time = format_timestamps_in_dict(dict_alert_time)\n",
    "\n",
    "# fill the report with the missing sensors dict, with one line per installation, and a list of the missing sensors\n",
    "for instal in dict_instal_json:\n",
    "    df_report.loc[instal, (\"Installation\")] = instal\n",
    "    df_report.loc[instal, (\"Sensor ID\")] = df_missing_sensors[\n",
    "        df_missing_sensors[\"installation\"] == instal\n",
    "    ].address.to_list()\n",
    "    df_report.loc[instal, (\"Sensor Name\")] = df_missing_sensors[\n",
    "        df_missing_sensors[\"installation\"] == instal\n",
    "    ].sensor_name.to_list()\n",
    "    df_report.loc[instal, (\"Snow fall\")] = dict_alert_time[instal][\"Snow fall\"]\n",
    "    df_report.loc[instal, (\"Strong wind\")] = dict_alert_time[instal][\"Strong wind\"]\n",
    "    df_report.loc[instal, (\"High temp\")] = dict_alert_time[instal][\"High temperature\"]\n",
    "    df_report.loc[instal, (\"Screen mode\")] = dict_screen_states[instal]\n",
    "df_report = df_report.reset_index(drop=True)\n",
    "\n",
    "#make a copy of the report dataframe to convert the lists to strings, so that it can be printed\n",
    "df_report_string = df_report.copy()\n",
    "\n",
    "# convert the lists to strings\n",
    "for col in df_report_string.columns:\n",
    "    if col == \"Installation\":\n",
    "        continue\n",
    "    df_report_string[col] = df_report_string[col].apply(list_to_string)\n",
    "\n",
    "\n",
    "save_alerts_to_csv(df_report_string.copy())\n",
    "\n",
    "no_weather_data = \"\"\n",
    "loc_no_weather = \"\"\n",
    "for instal in dict_instal_json:\n",
    "    if dict_weather_data[instal][\"list\"][0][\"dt\"] == 0:\n",
    "        no_weather_data = \"\\n⚠️ no weather data available for \"\n",
    "        loc_no_weather = loc_no_weather + instal + \", \"\n",
    "if len(no_weather_data) > 0:\n",
    "    print(no_weather_data + loc_no_weather[:-2] + \"\\n\")\n",
    "\n",
    "\n",
    "print(tabulate(df_report_string, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the last logs for each installation and sensor\n",
    "last_logs(dict_instal_json, list_sensor, api)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataViz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
