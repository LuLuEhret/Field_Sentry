{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FieldSentry.py is a script that checks the status of the sensors, weather and screens of the installations\n",
    "from insolAPI.WebAPI import API\n",
    "import simplejson as json\n",
    "import pandas as pd\n",
    "import pendulum as pdl\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import requests\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "# import datetime\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "import pytz\n",
    "import getpass\n",
    "import datetime as dt\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "SHOW_PLOT = False\n",
    "CUSTOM_DATE = True\n",
    "\n",
    "\n",
    "list_sensor = [\n",
    "    \"PAR\",\n",
    "    \"IRRAD\",\n",
    "    \"GII\",\n",
    "    \"DNI\",\n",
    "    \"DHI\",\n",
    "    \"TEMP\",\n",
    "    \"HUMI\",\n",
    "    \"RAIN\",\n",
    "    \"RAIN_TYPE\",\n",
    "    \"RAIN_ACCUMULATED\",\n",
    "    \"WIND\",\n",
    "    \"WIND_DIR\",\n",
    "    \"VIRTUAL\",\n",
    "    \"LEAF_TEMP\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "dict_screen_mode = {\n",
    "    0: \"\",\n",
    "    1: \"Auto\",\n",
    "    2: \"Manual\",\n",
    "    3: \"Emergency\",\n",
    "    4: \"Protection\",\n",
    "    5: \"Demo\",\n",
    "    6: \"Remote\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def read_json_config():\n",
    "    \"\"\"\n",
    "    Read the config.json file and return the api key\n",
    "    Parameters and response: https://openweathermap.org/forecast5\n",
    "    \"\"\"\n",
    "    with open(\"config/config.json\") as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    installations = {}\n",
    "\n",
    "    # Iterate through locations\n",
    "    locations = config_data['locations']\n",
    "    for location in locations:\n",
    "        # Store details in a dictionary\n",
    "        location_details = {\n",
    "            'id': location['id'],\n",
    "            'name': location['name'],\n",
    "            'latitude': location['latitude'],\n",
    "            'longitude': location['longitude'],\n",
    "            'wind_threshold': location['wind_threshold'],\n",
    "            'high_temperature_threshold': location['high_temperature_threshold'],\n",
    "            'has_a_screen': location['has_a_screen'],\n",
    "        }\n",
    "        installations[location['name']] = location_details\n",
    "    return installations, [config_data['api_key'], config_data['api_url']]\n",
    "\n",
    "\n",
    "\n",
    "def format_timestamp(original_timestamp_str):\n",
    "    original_timestamp = datetime.strptime(original_timestamp_str, '%Y-%m-%d %H:%M:%S')\n",
    "    formatted_timestamp_str = original_timestamp.strftime('%Hh %d-%m-%Y')\n",
    "    return formatted_timestamp_str\n",
    "\n",
    "\n",
    "\n",
    "def format_timestamps_in_dict(input_dict):\n",
    "    formatted_dict = {}\n",
    "    for key, sub_dict in input_dict.items():\n",
    "        formatted_dict[key] = {}\n",
    "        for event, timestamps in sub_dict.items():\n",
    "            try:\n",
    "                formatted_timestamps = [format_timestamp(ts) for ts in timestamps]\n",
    "                formatted_dict[key][event] = formatted_timestamps\n",
    "            except:\n",
    "                formatted_dict[key][event] = timestamps\n",
    "                pass\n",
    "    return formatted_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_weather_forecast(dict_instal, api_data, city_name):\n",
    "    \"\"\"\n",
    "    Request the weather forecast for the next 2 days, every 3 hours\n",
    "    \"\"\"\n",
    "    params = {\"appid\": api_data[0], \"cnt\": \"20\", \"units\": \"metric\"}\n",
    "\n",
    "    params[\"lat\"] = dict_instal[city_name][\"latitude\"]\n",
    "    params[\"lon\"] = dict_instal[city_name][\"longitude\"]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(api_data[1], params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # update_request_count()\n",
    "        return data\n",
    "\n",
    "    except Exception as ex:\n",
    "        # print(f\" \\n Error: {ex}\")\n",
    "        #generate an empty dict to avoid errors\n",
    "        error_dict = {'list': [{'dt': 0,\n",
    "                    'main': {'temp': 0,\n",
    "                        'feels_like': 0,\n",
    "                        'temp_min': 0,\n",
    "                        'temp_max': 0,\n",
    "                        'pressure': 0,\n",
    "                        'sea_level': 0,\n",
    "                        'grnd_level': 0,\n",
    "                        'humidity': 0,\n",
    "                        'temp_kf': 0},\n",
    "                    'weather': [{'id': 0,\n",
    "                        'main': 'Rain',\n",
    "                        'description': 'light rain',\n",
    "                        'icon': '10d'}],\n",
    "                    'clouds': {'all': 0},\n",
    "                    'wind': {'speed': 0, 'deg': 0, 'gust': 0},\n",
    "                    'visibility': 0,\n",
    "                    'pop': 0,\n",
    "                    'rain': {'3h': 0},\n",
    "                    'sys': {'pod': 'd'},\n",
    "                    'dt_txt': '9999-01-01 00:00:00'}]}\n",
    "        return error_dict\n",
    "\n",
    "\n",
    "\n",
    "def plot_weather_forecast(weather_data, city_name):\n",
    "    \"\"\"\n",
    "    If called, plot the weather forecast for the next 2 days\n",
    "    \"\"\"\n",
    "    forecast_date = []\n",
    "    forecast_temp = []\n",
    "    forecast_snow = []\n",
    "    forecast_rain = []\n",
    "    forecast_wind = []\n",
    "    forecast_pop = []\n",
    "\n",
    "    for forecast in weather_data[\"list\"]:\n",
    "        forecast_date.append(forecast[\"dt_txt\"])\n",
    "        forecast_temp.append(forecast[\"main\"][\"temp\"])\n",
    "        forecast_wind.append(forecast[\"wind\"][\"speed\"])\n",
    "        forecast_pop.append(forecast[\"pop\"])\n",
    "        try:\n",
    "            forecast_snow.append(forecast[\"snow\"][\"3h\"])\n",
    "        except:\n",
    "            forecast_snow.append(0)\n",
    "        try:\n",
    "            forecast_rain.append(forecast[\"rain\"][\"3h\"])\n",
    "        except:\n",
    "            forecast_rain.append(0)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecast_date,\n",
    "            y=forecast_temp,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Temperature\",\n",
    "            line=dict(color=\"orange\", width=2),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecast_date,\n",
    "            y=forecast_snow,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Snow\",\n",
    "            line=dict(color=\"lightblue\", width=2),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecast_date,\n",
    "            y=forecast_rain,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Rain\",\n",
    "            line=dict(color=\"darkcyan\", width=2),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecast_date,\n",
    "            y=forecast_wind,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Wind\",\n",
    "            line=dict(color=\"darkred\", width=2),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=forecast_date,\n",
    "            y=forecast_pop,\n",
    "            name=\"Precipitation Probability\",\n",
    "            marker_color=\"lightgrey\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(\n",
    "        title=\"Weather Forecast for \" + city_name + \"\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Values\",\n",
    "        legend=dict(x=1, y=1, traceorder=\"normal\"),\n",
    "        # Background color of the entire graph area,\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor=\"rgba(0,0,0,0.1)\")\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor=\"rgba(0,0,0,0.1)\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "def alert_user(weather_data, dict_events):\n",
    "    \"\"\"\n",
    "    Set the threshold for the alerts, and which alerts to send\n",
    "    \"\"\"\n",
    "    alert_list = []\n",
    "    for forecast in weather_data[\"list\"]:\n",
    "        try:\n",
    "            if bool(forecast[\"snow\"][\"3h\"]):\n",
    "                alert_list.append(\"Snow fall\")\n",
    "                alert_list.append(forecast[\"dt_txt\"])\n",
    "        except:\n",
    "            pass\n",
    "        if forecast[\"wind\"][\"speed\"] > dict_events[\"wind_threshold\"]:\n",
    "            alert_list.append(\"Strong wind\")\n",
    "            alert_list.append(forecast[\"dt_txt\"])\n",
    "        if forecast[\"main\"][\"temp\"] > dict_events[\"high_temperature_threshold\"]:\n",
    "            alert_list.append(\"High temperature\")\n",
    "            alert_list.append(forecast[\"dt_txt\"])\n",
    "        #to add a new alert, add the condition here. Also need to change the main\n",
    "    # if forecast[\"dt\"] == 0:\n",
    "    #     print(\"\\nError: no weather data available\")\n",
    "    return alert_list\n",
    "\n",
    "\n",
    "\n",
    "def update_request_count():\n",
    "    \"\"\"\n",
    "    update the number of requests made to the API, and the number of requests made during the last hour\n",
    "    \"\"\"\n",
    "    file_name = \"reports/count_requests.csv\"\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_name):\n",
    "        # Read the last line and get the count\n",
    "        with open(file_name, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines:\n",
    "                last_line = lines[-1].strip()\n",
    "                count = int(last_line.split(\",\")[1]) + 1\n",
    "            else:\n",
    "                count = 1\n",
    "            # add the number of requests made during the last hour\n",
    "            last_hour = datetime.now() - datetime.timedelta(hours=1)\n",
    "            count_last_hour = 1\n",
    "            for line in lines:\n",
    "                if (\n",
    "                    datetime.strptime(\n",
    "                        line.split(\",\")[4].strip(), \"%Y-%m-%d %H:%M:%S\"\n",
    "                    )\n",
    "                    >= last_hour\n",
    "                ):\n",
    "                    count_last_hour = count_last_hour + 1\n",
    "    else:\n",
    "        # If the file doesn't exist, create it and set count to 1\n",
    "        count = 1\n",
    "        count_last_hour = 1\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    with open(file_name, \"a\") as file:\n",
    "        file.write(f\"Total,{count},last hour,{count_last_hour},{timestamp}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def save_alerts_to_csv(df):\n",
    "    \"\"\"\n",
    "    update the csv file with the alerts\n",
    "    \"\"\"\n",
    "    df['Timestamp'] = datetime.now().strftime('%H:%M:%S %d-%m-%Y')\n",
    "\n",
    "    #replace \\n with a \",\"\n",
    "    df = df.replace('\\n', ', ', regex=True)\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    df = df[[cols[-1]] + cols[:-1]]\n",
    "\n",
    "\n",
    "    #check if the file exists\n",
    "    if not os.path.exists(\"reports/log_reports.csv\"):\n",
    "        df.to_csv(\"reports/log_reports.csv\", index=False, header=True) # add the header if the file doesn't exist\n",
    "    else:\n",
    "        df.to_csv(\"reports/log_reports.csv\", mode='a', header=False, index=False)\n",
    "        with open(\"reports/log_reports.csv\", \"a\") as file:\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def list_to_string(lst):\n",
    "    \"\"\"\n",
    "    Convert a list to a string, to be able to print it\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return \"\\n\".join(map(str, lst))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def process_screen_data(df):\n",
    "    \"\"\"\n",
    "    logics to process the screen data\n",
    "    \"\"\"\n",
    "    df = df[['screen_id', 'state', 'name']]\n",
    "    screen_names = df['name'].unique()\n",
    "    list_states = []\n",
    "    for screen_name in screen_names:\n",
    "        df_screen_tmp = df[df['name'] == screen_name]\n",
    "        df_screen_tmp = df.sort_index(ascending=False)\n",
    "        state = df_screen_tmp['state'][0]\n",
    "        if state != 1:\n",
    "            list_states.append(f\"{screen_name}: {dict_screen_mode[state]}\")\n",
    "    if screen_names.size == 0:\n",
    "        list_states.append(\"No logs for 2d+\")\n",
    "    return list_states\n",
    "\n",
    "\n",
    "\n",
    "def print_progress_bar(percentage, length=10):\n",
    "    # print(percentage)\n",
    "    if np.isnan(percentage):\n",
    "        percentage = 1\n",
    "    block = int(round(length * percentage))\n",
    "    progress = \"[\" + \"#\" * block + \"-\" * (length - block) + \"]\"\n",
    "    # print(f\"\\r{progress}\", end=\"\", flush=True)\n",
    "    return progress\n",
    "\n",
    "\n",
    "\n",
    "def last_logs(dict_instal, list_sensor, api):\n",
    "    time_args = dict(\n",
    "        start=pdl.yesterday().subtract(weeks=1).to_datetime_string(),\n",
    "        stop=(pdl.now()).to_datetime_string(),\n",
    "        timezone = timezone('Europe/Zurich')\n",
    "    )\n",
    "\n",
    "    logs_joined = {}\n",
    "    dict_list_theoretical = {}\n",
    "    unique_sensors = {}\n",
    "    logs_joined_unique = {}\n",
    "    last_log = {}\n",
    "    time_diff = {}\n",
    "    print(\"\\nCollecting data...\\n\")\n",
    "    for instal in tqdm(dict_instal):\n",
    "        logs_joined[instal] = {}\n",
    "        dict_list_theoretical[instal] = []\n",
    "        sensor_number = 0\n",
    "        if dict_instal[instal][\"id\"] == \"xx\":\n",
    "            continue\n",
    "        for sensor_type in api.SensorsTypes:\n",
    "            if str(sensor_type).split(\".\")[1] in list_sensor:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                    logs_joined[instal][list_sensor[sensor_number]] = api.get_sensor_channels_logs_joined(**time_args,sensor_type=sensor_type, install=dict_instal[instal][\"id\"])\n",
    "                    sensor_number += 1\n",
    "                    try:\n",
    "                        theoretical_sensor = api.get_sensor_channels(sensor_type=sensor_type, install=dict_instal[instal][\"id\"])\n",
    "                        theoretical_sensor = theoretical_sensor[theoretical_sensor[\"deleted_at\"].isna()].sensor_name.unique()\n",
    "                        dict_list_theoretical[instal].extend(theoretical_sensor)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "    print(\"\\nProcessing data...\\n\")\n",
    "    for instal in dict_instal:\n",
    "        unique_sensors[instal] = []\n",
    "        time_diff[instal] = {}\n",
    "        for sensor_type in logs_joined[instal]:\n",
    "            try:\n",
    "                unique_sensors[instal].extend(logs_joined[instal][sensor_type][\"sensor_name\"].unique())\n",
    "            except :\n",
    "                pass\n",
    "        unique_sensors[instal] = list(set(unique_sensors[instal]))\n",
    "        logs_joined_unique[instal] = {}\n",
    "        for sensor_type in logs_joined[instal]:\n",
    "            for unique_sensor in unique_sensors[instal]:\n",
    "                try:\n",
    "                    if unique_sensor in logs_joined[instal][sensor_type][\"sensor_name\"].unique():\n",
    "                        logs_joined_unique[instal][unique_sensor] = logs_joined[instal][sensor_type].loc[logs_joined[instal][sensor_type][\"sensor_name\"] == unique_sensor]\n",
    "                        logs_joined_unique[instal][unique_sensor].index = logs_joined_unique[instal][unique_sensor].index.round('min')\n",
    "                        logs_joined_unique[instal][unique_sensor] = logs_joined_unique[instal][unique_sensor].loc[~logs_joined_unique[instal][unique_sensor].index.duplicated(keep='first')]\n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "\n",
    "        for sensor in dict_list_theoretical[instal]:\n",
    "            try:\n",
    "                logs_joined_unique[instal][sensor] = logs_joined_unique[instal][sensor].dropna(subset=[logs_joined_unique[instal][sensor].columns[1]])\n",
    "                time_serie = logs_joined_unique[instal][sensor].index.tz_localize(None)\n",
    "                now = pd.to_datetime(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                now_serie = pd.Series(data=now, index=[now])\n",
    "\n",
    "                time_serie.to_series()\n",
    "                time_serie = pd.Series(time_serie)\n",
    "\n",
    "                series = [time_serie, now_serie]\n",
    "                time_series = pd.concat(series, ignore_index=True)\n",
    "                time_difference = time_series.diff()\n",
    "                time_difference = time_difference[time_difference > pd.Timedelta(minutes=2)].sum()\n",
    "                time_diff[instal][sensor] = time_difference\n",
    "            except:\n",
    "                time_diff[instal][sensor] = pd.NaT\n",
    "\n",
    "        last_log[instal] = {}\n",
    "        for sensor in dict_list_theoretical[instal]:\n",
    "            try:\n",
    "                last_log[instal][sensor] = logs_joined_unique[instal][sensor].index[-1].strftime(\"%Y-%m-%d %Hh%M\")\n",
    "            except:\n",
    "                last_log[instal][sensor] = \"> 1 week\"\n",
    "            try:\n",
    "                if logs_joined_unique[instal][sensor].index[-1] > pdl.now().subtract(minutes=10):\n",
    "                    last_log[instal][sensor] = \"Online\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    #make a df with the last log and the time difference for each sensor\n",
    "    dict_df = {}\n",
    "    for instal in last_log:\n",
    "        dict_df[instal] = pd.DataFrame.from_dict(last_log[instal], orient=\"index\", columns=[\"Last log\"])\n",
    "        dict_df[instal][\"Time offline (1w)\"] = dict_df[instal].index.map(time_diff[instal])\n",
    "        try:\n",
    "            dict_df[instal][\"% offline\"] = dict_df[instal][\"Time offline (1w)\"].apply(lambda x: print_progress_bar(x.total_seconds() / (7 * 24 * 60 * 60)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    for instal, df in dict_df.items():\n",
    "        # Sort the DataFrame by the \"Last log\" column\n",
    "        df_sorted = df.sort_values(by=[\"Last log\", \"Time offline (1w)\"], ascending=[True, False])\n",
    "\n",
    "        print(f\"{instal}\")\n",
    "        table = tabulate(df_sorted, headers=\"keys\", tablefmt=\"psql\", showindex=True)\n",
    "        print(table)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        with open(\"reports/output.txt\", \"a\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(f\"{instal}\\n\")\n",
    "            text_file.write(tabulate(df_sorted, headers=\"keys\", tablefmt=\"psql\", showindex=True))\n",
    "            text_file.write(\"\\n\\n\")\n",
    "    return logs_joined_unique\n",
    "\n",
    "\n",
    "\n",
    "def time_args_definition(start_date, stop_date):\n",
    "    time_args = dict(\n",
    "    start=str(start_date),\n",
    "    stop=str(stop_date),\n",
    "    timezone=timezone('Europe/Zurich'),\n",
    "    )\n",
    "    return time_args\n",
    "\n",
    "\n",
    "\n",
    "def validate_date(date_str):\n",
    "    #1st split the date string into year, month and day\n",
    "    #convert date_parts and time_parts into integers\n",
    "    try:\n",
    "        res = bool(parser.parse(date_str))\n",
    "    except ValueError:\n",
    "        res = False\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def ask_for_datetime(date_input, show_details = False, deltaT_days=0, deltaT_hours=0, deltaT_minutes=30):\n",
    "    # Prompt user to enter a date\n",
    "    # date_input = input(\"Enter a date (yyyy-mm-dd HH:MM): \")\n",
    "\n",
    "    # Validate the date format\n",
    "    if validate_date(date_input):\n",
    "        #convert date_input into datetime object\n",
    "        date_obj = parser.parse(date_input)\n",
    "        if date_obj <= datetime.now():\n",
    "            if date_obj.year >= 2022:\n",
    "                if show_details: print(f\"start date is: {date_obj}\")\n",
    "                # add 30min  to the date\n",
    "                new_date = date_obj + timedelta(days=deltaT_days,hours=deltaT_hours, minutes=deltaT_minutes)\n",
    "                if new_date <= datetime.now():\n",
    "                    if show_details: print(f\"Stop date is: {new_date}\")\n",
    "                    time_args_temp = time_args_definition(date_obj, new_date)\n",
    "                    return time_args_temp\n",
    "                else:\n",
    "                    print(\"The stop date is in the future\")\n",
    "                    input_date = input(\"Enter a date (yyyy-mm-dd HH:MM): \")\n",
    "                    return ask_for_datetime(input_date)\n",
    "            else:\n",
    "                print(\"The year should be greater than 2022\")\n",
    "                input_date = input(\"Enter a date (yyyy-mm-dd HH:MM): \")\n",
    "                return ask_for_datetime(input_date)\n",
    "        else:\n",
    "            print(\"The start date is in the future\")\n",
    "            input_date = input(\"Enter a date (yyyy-mm-dd HH:MM): \")\n",
    "            return ask_for_datetime(input_date)\n",
    "    else:\n",
    "        print(\"Invalid date format. Please enter date in yyyy-mm-dd HH:MM format.\")\n",
    "        input_date = input(\"Enter a date (yyyy-mm-dd HH:MM): \")\n",
    "        return ask_for_datetime(input_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "## Collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to the API\n",
      "Collecting data...\n",
      "\n",
      "The start date is in the future\n",
      "Invalid date format. Please enter date in yyyy-mm-dd HH:MM format.\n",
      "Invalid date format. Please enter date in yyyy-mm-dd HH:MM format.\n",
      "The start date is in the future\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:13<00:08,  4.26s/it]"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"config/api_credits_path.json\") as f:\n",
    "        installation_path = json.load(f)[\"path\"]\n",
    "\n",
    "    # installation_path = \"C:/Users/Insolight/Desktop/InsolReports/Installations/\"\n",
    "    with open(installation_path + \"local.json\") as f:\n",
    "        local_data = json.load(f)\n",
    "\n",
    "    api = API(local_data[\"API_user\"], local_data[\"API_pwd\"], dev_space=False)\n",
    "    api.get_sensor_channels(sensor_type=api.SensorsTypes.TEMP, install=23)\n",
    "    print(\"✅ Successfully connected to the API\\nCollecting data...\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ {e}\\nNot Connected to the API\")\n",
    "    print(\"\\nExiting...\")\n",
    "    sys.stdout.flush()\n",
    "    sys.exit(0)\n",
    "\n",
    "if CUSTOM_DATE:\n",
    "    date_input = input(\"Enter a date (yyyy-mm-dd HH:MM): \")\n",
    "    time_args = ask_for_datetime(date_input, True)\n",
    "    time_args_screens = ask_for_datetime(date_input, False, 2, 0, 0)\n",
    "else:\n",
    "    time_args = dict(\n",
    "    start=pdl.now().subtract(days=0, hours=0, minutes=30).to_datetime_string(),\n",
    "    stop=pdl.now().subtract(days=0, hours=0).to_datetime_string(),\n",
    "    timezone=timezone('Europe/Zurich'),\n",
    "    )\n",
    "    time_args_screens = dict(\n",
    "        start=pdl.now().subtract(days=2, hours=0, minutes=0).to_datetime_string(),\n",
    "        stop=pdl.now().subtract(days=0, hours=0).to_datetime_string(),\n",
    "        timezone=timezone('Europe/Zurich'),\n",
    "    )\n",
    "\n",
    "# declarations of the dictionaries\n",
    "dict_instal_json, api_data = read_json_config()\n",
    "dict_instal_logs = {}  # type: dict\n",
    "dict_sensor_channel_id = {}  # type: dict\n",
    "diff_logs = {}  # type: dict\n",
    "dict_logs_joined = {}  # type: dict\n",
    "dict_channel_id = {}  # type: dict\n",
    "dict_missing_sensors = {}  # type: dict\n",
    "dict_weather_data = {} # type: dict\n",
    "dict_alerts = {} # type: dict\n",
    "dict_time_of_snow = {} # type: dict\n",
    "dict_time_of_wind = {} # type: dict\n",
    "dict_time_high_T = {} # type: dict\n",
    "dict_alert_time = {} # type: dict\n",
    "dict_screen_states = {} # type: dict\n",
    "\n",
    "#main loop\n",
    "for instal in tqdm(dict_instal_json):\n",
    "    list_channel_id = [] # type: list\n",
    "    list_sensor_logging = [] # type: list\n",
    "    dict_logs_joined[instal] = {}\n",
    "    dict_sensor_channel_id[instal] = {}\n",
    "    sensor_number = 0\n",
    "    dict_alert_time[instal] = {\"Snow fall\": [], \"Strong wind\": [], \"High temperature\": []}\n",
    "    list_snow_time = []\n",
    "    list_wind_time = []\n",
    "    list_highT_time = []\n",
    "    dict_df_screen = {}\n",
    "    list_screen_states = []\n",
    "    diff_logs[instal] = []\n",
    "    dict_screen_states[instal] = []\n",
    "\n",
    "\n",
    "    # get the weather forecast for each installation\n",
    "    dict_weather_data[instal] = get_weather_forecast(dict_instal_json, api_data, instal)\n",
    "    if SHOW_PLOT:\n",
    "        plot_weather_forecast(dict_weather_data[instal], instal)\n",
    "    dict_alerts[instal] = alert_user(dict_weather_data[instal], dict_instal_json[instal])\n",
    "\n",
    "    for i in range(0, len(dict_alerts[instal]), 2):\n",
    "        if dict_alerts[instal][i] == \"Snow fall\":\n",
    "            list_snow_time.append(dict_alerts[instal][i + 1])\n",
    "            dict_alert_time[instal][\"Snow fall\"].append(dict_alerts[instal][i + 1])\n",
    "        if dict_alerts[instal][i] == \"Strong wind\":\n",
    "            list_wind_time.append(dict_alerts[instal][i + 1])\n",
    "            dict_alert_time[instal][\"Strong wind\"].append(dict_alerts[instal][i + 1])\n",
    "        if dict_alerts[instal][i] == \"High temperature\":\n",
    "            list_highT_time.append(dict_alerts[instal][i + 1])\n",
    "            dict_alert_time[instal][\"High temperature\"] = dict_alerts[instal][i + 1]\n",
    "\n",
    "    if dict_instal_json[instal][\"id\"] == 'xx':\n",
    "        continue\n",
    "\n",
    "\n",
    "    #screen data\n",
    "    if dict_instal_json[instal][\"has_a_screen\"]:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            dict_df_screen[instal] = api.get_screens_logs_joined(**time_args_screens, install=dict_instal_json[instal][\"id\"])\n",
    "            list_screen_states = process_screen_data(dict_df_screen[instal])\n",
    "    dict_screen_states[instal] = list_screen_states\n",
    "\n",
    "\n",
    "    # get all the sensors and channels for each installation\n",
    "    for sensor_type in api.SensorsTypes:\n",
    "        if str(sensor_type).split(\".\")[1] in list_sensor:\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "                    dict_logs_joined[instal][\n",
    "                        list_sensor[sensor_number]\n",
    "                    ] = api.get_sensor_channels_logs_joined(\n",
    "                        **time_args,\n",
    "                        sensor_type=sensor_type,\n",
    "                        install=dict_instal_json[instal][\"id\"],\n",
    "                    )\n",
    "                    list_sensor_logging.extend(\n",
    "                        dict_logs_joined[instal][\n",
    "                            list_sensor[sensor_number]\n",
    "                        ].sensor_channel_id.unique()\n",
    "                    )\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "                    dict_sensor_channel_id[instal][\n",
    "                        list_sensor[sensor_number]\n",
    "                    ] = api.get_sensor_channels(\n",
    "                        sensor_type=sensor_type, install=dict_instal_json[instal][\"id\"]\n",
    "                    )\n",
    "\n",
    "                    list_channel_id.extend(\n",
    "                        dict_sensor_channel_id[instal][list_sensor[sensor_number]][dict_sensor_channel_id[instal][list_sensor[sensor_number]][\"deleted_at\"].isna()].index.tolist()\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            sensor_number += 1\n",
    "\n",
    "    dict_instal_logs[instal] = list_sensor_logging  # dict with the list of sensors logging for each installation\n",
    "    dict_channel_id[instal] = list_channel_id       # dict with the list of channels for each installation\n",
    "\n",
    "\n",
    "    # substraction, result is the list of sensors that are not logging, and not deleted from the config\n",
    "    diff_logs[instal] = list(\n",
    "        set(dict_channel_id[instal]) - set(dict_instal_logs[instal])\n",
    "    )\n",
    "\n",
    "# download the logs for the preseries for 1 day\n",
    "with open(installation_path + \"local.json\") as f:\n",
    "    local_data = json.load(f)\n",
    "\n",
    "api_razon = API(local_data[\"API_user\"], local_data[\"API_pwd\"], dev_space=False, install=9)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    start_time = dt.datetime.now() - dt.timedelta(hours=2)\n",
    "    df_razon = api_razon.get_sensors_csv(start=start_time);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presenting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 61.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ Qair has no ID, please edit the config.json file ⚠️\n",
      "\n",
      "2024-02-07 14h53 - 👤 Insolight\n",
      "\n",
      "+-------------------+-------------+------------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Installation      | Sensor ID   | Sensor Name                              | Snow fall   | Strong wind    | High temp   | Screen mode     |\n",
      "+===================+=============+==========================================+=============+================+=============+=================+\n",
      "| Agroscope Series  |             |                                          |             |                |             |                 |\n",
      "+-------------------+-------------+------------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Agroscope Berries | 86          | WIND02_EXT                               |             |                |             | No logs for 2d+ |\n",
      "|                   | 43          | TRH_top_Insolagrin01_T60_Portrait_EW     |             |                |             |                 |\n",
      "|                   | 44          | TRH_bottom_Insolagrin01_T60_Portrait_EW  |             |                |             |                 |\n",
      "|                   | 45          | TRH_top_Insolagrin02_T40_Portrait_EW     |             |                |             |                 |\n",
      "|                   | 46          | TRH_bottom_Insolagrin02_T40_Portrait_EW  |             |                |             |                 |\n",
      "|                   | 13          | PAR01_Insolagrin01_T60_Portrait_EW       |             |                |             |                 |\n",
      "|                   | 14          | PAR02_Insolagrin01_T60_Portrait_EW       |             |                |             |                 |\n",
      "|                   | 15          | PAR03_Insolagrin01_T60_Portrait_EW       |             |                |             |                 |\n",
      "|                   | 16          | PAR04_Insolagrin01_T60_Portrait_EW       |             |                |             |                 |\n",
      "|                   | 17          | PAR01_Insolagrin02_T40_Portrait_EW       |             |                |             |                 |\n",
      "|                   | 18          | PAR02_Insolagrin02_T40_Portrait_EW       |             |                |             |                 |\n",
      "|                   | 19          | PAR03_Insolagrin02_T40_Portrait_EW       |             |                |             |                 |\n",
      "|                   | 20          | PAR04_Insolagrin02_T40_Portrait_EW       |             |                |             |                 |\n",
      "|                   | 47          | TRH_top_Insolagrin03_T60_Portrait_WW     |             |                |             |                 |\n",
      "|                   | 48          | TRH_bottom_Insolagrin03_T60_Portrait_WW  |             |                |             |                 |\n",
      "|                   | 49          | TRH_top_Insolagrin04_T40_Portrait_WW     |             |                |             |                 |\n",
      "|                   | 50          | TRH_bottom_Insolagrin04_T40_Portrait_WW  |             |                |             |                 |\n",
      "|                   | 21          | PAR01_Insolagrin03_T60_Portrait_WW       |             |                |             |                 |\n",
      "|                   | 22          | PAR02_Insolagrin03_T60_Portrait_WW       |             |                |             |                 |\n",
      "|                   | 23          | PAR03_Insolagrin03_T60_Portrait_WW       |             |                |             |                 |\n",
      "|                   | 24          | PAR04_Insolagrin03_T60_Portrait_WW       |             |                |             |                 |\n",
      "|                   | 25          | PAR01_Insolagrin04_T40_Portrait_WW       |             |                |             |                 |\n",
      "|                   | 26          | PAR02_Insolagrin04_T40_Portrait_WW       |             |                |             |                 |\n",
      "|                   | 27          | PAR03_Insolagrin04_T40_Portrait_WW       |             |                |             |                 |\n",
      "|                   | 28          | PAR04_Insolagrin04_T40_Portrait_WW       |             |                |             |                 |\n",
      "|                   | 11          | PAR01_EXT                                |             |                |             |                 |\n",
      "|                   | 12          | PAR02_EXT                                |             |                |             |                 |\n",
      "|                   | 41          | TRH_top_EXT                              |             |                |             |                 |\n",
      "|                   | 42          | TRH_bottom_EXT                           |             |                |             |                 |\n",
      "|                   | 52          | TRH_bottom_Insolagrin05_T60_Landscape_EW |             |                |             |                 |\n",
      "|                   | 51          | TRH_top_Insolagrin05_T60_Landscape_EW    |             |                |             |                 |\n",
      "|                   | 54          | TRH_bottom_Insolagrin06_T40_Landscape_EW |             |                |             |                 |\n",
      "|                   | 53          | TRH_top_Insolagrin06_T40_Landscape_EW    |             |                |             |                 |\n",
      "|                   | 29          | PAR01_Insolagrin05_T60_Landscape_EW      |             |                |             |                 |\n",
      "|                   | 30          | PAR02_Insolagrin05_T60_Landscape_EW      |             |                |             |                 |\n",
      "|                   | 31          | PAR03_Insolagrin05_T60_Landscape_EW      |             |                |             |                 |\n",
      "|                   | 32          | PAR04_Insolagrin05_T60_Landscape_EW      |             |                |             |                 |\n",
      "|                   | 33          | PAR01_Insolagrin06_T40_Landscape_EW      |             |                |             |                 |\n",
      "|                   | 34          | PAR02_Insolagrin06_T40_Landscape_EW      |             |                |             |                 |\n",
      "|                   | 35          | PAR03_Insolagrin06_T40_Landscape_EW      |             |                |             |                 |\n",
      "|                   | 36          | PAR04_Insolagrin06_T40_Landscape_EW      |             |                |             |                 |\n",
      "|                   | 130         | LeafTemp_Z2_int_int                      |             |                |             |                 |\n",
      "|                   | 131         | LeafTemp_Z2_side_int                     |             |                |             |                 |\n",
      "|                   | 132         | LeafTemp_Z2_side_ext                     |             |                |             |                 |\n",
      "|                   | 137         | LeafTemp_Z4_int_int                      |             |                |             |                 |\n",
      "|                   | 161         | SOIL_UP_Insolagrin02                     |             |                |             |                 |\n",
      "|                   | 162         | SOIL_DOWN_Insolagrin02                   |             |                |             |                 |\n",
      "|                   | 117         | RAIN_EXT                                 |             |                |             |                 |\n",
      "|                   | 71          | WIND01_EXT                               |             |                |             |                 |\n",
      "+-------------------+-------------+------------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Bioschmid         |             |                                          |             |                |             | No logs for 2d+ |\n",
      "+-------------------+-------------+------------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Etchelecu         |             |                                          |             |                |             | No logs for 2d+ |\n",
      "+-------------------+-------------+------------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Qair              |             |                                          |             | 00h 09-02-2024 |             |                 |\n",
      "|                   |             |                                          |             | 03h 09-02-2024 |             |                 |\n",
      "|                   |             |                                          |             | 09h 09-02-2024 |             |                 |\n",
      "|                   |             |                                          |             | 12h 09-02-2024 |             |                 |\n",
      "|                   |             |                                          |             | 15h 09-02-2024 |             |                 |\n",
      "|                   |             |                                          |             | 18h 09-02-2024 |             |                 |\n",
      "+-------------------+-------------+------------------------------------------+-------------+----------------+-------------+-----------------+\n",
      "| Pre-Serie         |             |                                          |             |                |             |                 |\n",
      "+-------------------+-------------+------------------------------------------+-------------+----------------+-------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for instal in tqdm(dict_instal_json):\n",
    "    #try to identify the missing sensors from the channel list\n",
    "    for sensor_id in diff_logs[instal]:\n",
    "        for sensor in list_sensor:\n",
    "            try:\n",
    "                dict_missing_sensors[sensor_id] = [\n",
    "                    instal,\n",
    "                    dict_sensor_channel_id[instal][sensor]\n",
    "                    .loc[sensor_id]\n",
    "                    .address,\n",
    "                    dict_sensor_channel_id[instal][sensor]\n",
    "                    .loc[sensor_id]\n",
    "                    .sensor_name,\n",
    "                    dict_sensor_channel_id[instal][sensor]\n",
    "                    .loc[sensor_id]\n",
    "                    .channel_name,\n",
    "                ]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "df_missing_sensors = pd.DataFrame.from_dict(\n",
    "    dict_missing_sensors,\n",
    "    orient=\"index\",\n",
    "    columns=[\"installation\", \"address\", \"sensor_name\", \"channel_name\"],\n",
    ")\n",
    "df_missing_sensors = df_missing_sensors.drop_duplicates(subset=[\"sensor_name\"])\n",
    "\n",
    "columns = [\"Installation\", \"Sensor ID\", \"Sensor Name\", \"Snow fall\", \"Strong wind\", \"High temp\", \"Screen mode\"]\n",
    "df_report = pd.DataFrame(columns=columns)\n",
    "\n",
    "dict_alert_time = format_timestamps_in_dict(dict_alert_time)\n",
    "\n",
    "# fill the report with the missing sensors dict, with one line per installation, and a list of the missing sensors\n",
    "for instal in dict_instal_json:\n",
    "    df_report.loc[instal, (\"Installation\")] = instal\n",
    "    df_report.loc[instal, (\"Sensor ID\")] = df_missing_sensors[\n",
    "        df_missing_sensors[\"installation\"] == instal\n",
    "    ].address.to_list()\n",
    "    df_report.loc[instal, (\"Sensor Name\")] = df_missing_sensors[\n",
    "        df_missing_sensors[\"installation\"] == instal\n",
    "    ].sensor_name.to_list()\n",
    "    df_report.loc[instal, (\"Snow fall\")] = dict_alert_time[instal][\"Snow fall\"]\n",
    "    df_report.loc[instal, (\"Strong wind\")] = dict_alert_time[instal][\"Strong wind\"]\n",
    "    df_report.loc[instal, (\"High temp\")] = dict_alert_time[instal][\"High temperature\"]\n",
    "    df_report.loc[instal, (\"Screen mode\")] = dict_screen_states[instal]\n",
    "df_report = df_report.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#make a copy of the report dataframe to convert the lists to strings, so that it can be printed\n",
    "df_report_string = df_report.copy()\n",
    "\n",
    "# convert the lists to strings\n",
    "for col in df_report_string.columns:\n",
    "    if col == \"Installation\":\n",
    "        continue\n",
    "    df_report_string[col] = df_report_string[col].apply(list_to_string)\n",
    "\n",
    "\n",
    "# add the Razon to the report\n",
    "if len(df_razon[df_razon[\"Identifier\"] == \"1_1\"]):\n",
    "    # print(\"Razon has been logging\")\n",
    "    last_log_razon = df_razon[df_razon[\"Identifier\"] == \"1_1\"].index[-1]\n",
    "    now_timestamp = pd.Timestamp.now(tz=\"UTC\")\n",
    "    time_diff_razon = now_timestamp - last_log_razon\n",
    "    # print(f\"Time since last log: {time_diff_razon}\")\n",
    "    if time_diff_razon > pd.Timedelta(minutes=20):\n",
    "        # time_diff_razon.strftime(\"%d %H:%M:%S\")\n",
    "        time_diff_razon = time_diff_razon.floor(\"T\")\n",
    "        df_report_string.loc[len(df_report)] = [\"Pre-Serie\", \"1_1\", f\"Razon_GHI: {time_diff_razon}\", \"\", \"\", \"\", \"\"]\n",
    "    else:\n",
    "        df_report_string.loc[len(df_report)] = [\"Pre-Serie\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "else:\n",
    "    df_report_string.loc[len(df_report)] = [\"Pre-Serie\", \"1_1\", \"Razon_GHI: No logs for > 2h\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "\n",
    "\n",
    "save_alerts_to_csv(df_report_string.copy())\n",
    "\n",
    "no_weather_data = \"\"\n",
    "loc_no_weather = \"\"\n",
    "for instal in dict_instal_json:\n",
    "    if dict_weather_data[instal][\"list\"][0][\"dt\"] == 0:\n",
    "        no_weather_data = \"\\n⚠️ no weather data available for \"\n",
    "        loc_no_weather = loc_no_weather + instal + \", \"\n",
    "if len(no_weather_data) > 0:\n",
    "    print(no_weather_data + loc_no_weather[:-2] + \"\\n\")\n",
    "\n",
    "for instal in dict_instal_json:\n",
    "    if dict_instal_json[instal][\"id\"] == \"xx\":\n",
    "        name = dict_instal_json[instal][\"name\"]\n",
    "        print(f\"\\n⚠️ {name} has no ID, please edit the config.json file ⚠️\\n\")\n",
    "\n",
    "try:\n",
    "    username = getpass.getuser()\n",
    "except:\n",
    "    username = \"xx\"\n",
    "print(str(pdl.now().strftime(\"%Y-%m-%d %Hh%M\")) + f\" - 👤 {getpass.getuser()}\" +\"\\n\")\n",
    "if CUSTOM_DATE: print(f\"📅 {time_args['start']} -> {time_args['stop']}\")\n",
    "print(tabulate(df_report_string, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n",
    "with open(\"reports/output.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(str(pdl.now().strftime(\"%Y-%m-%d %Hh%M\")) + f\" - 👤 {getpass.getuser()}\" +\"\\n\\n\")\n",
    "    text_file.write(tabulate(df_report_string, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n",
    "    text_file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting data...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:49<00:00,  9.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data...\n",
      "\n",
      "Agroscope Series\n",
      "+-----------------------------------+------------+---------------------+--------------+\n",
      "|                                   | Last log   | Time offline (1w)   | % offline    |\n",
      "|-----------------------------------+------------+---------------------+--------------|\n",
      "| TRH02_GT_tunnel                   | Online     | 0 days 00:12:00     | [----------] |\n",
      "| TRH_bottom_Nursery                | Online     | 0 days 00:09:00     | [----------] |\n",
      "| TRH01_GT1_tunnel                  | Online     | 0 days 00:09:00     | [----------] |\n",
      "| TRH01_GT_tunnel                   | Online     | 0 days 00:09:00     | [----------] |\n",
      "| wind_speed_ultrasonic             | Online     | 0 days 00:09:00     | [----------] |\n",
      "| TRH02_GT1_tunnel                  | Online     | 0 days 00:06:00     | [----------] |\n",
      "| PAR4                              | Online     | 0 days 00:03:00     | [----------] |\n",
      "| PAR3                              | Online     | 0 days 00:03:00     | [----------] |\n",
      "| PAR2                              | Online     | 0 days 00:03:00     | [----------] |\n",
      "| PAR1                              | Online     | 0 days 00:03:00     | [----------] |\n",
      "| weather_station                   | Online     | 0 days 00:03:00     | [----------] |\n",
      "| TRH_top_Nursery                   | Online     | 0 days 00:03:00     | [----------] |\n",
      "| TRH01_TX_tunnel                   | Online     | 0 days 00:03:00     | [----------] |\n",
      "| T_RH_umbrella_bottom              | Online     | 0 days 00:03:00     | [----------] |\n",
      "| T_RH_umbrella_top                 | Online     | 0 days 00:03:00     | [----------] |\n",
      "| wind_speed                        | Online     | 0 days 00:03:00     | [----------] |\n",
      "| PAR_EXT_06                        | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR_EXT_05                        | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR_EXT_04                        | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR01_Nursery                     | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR01_GT1_tunnel                  | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR01_GT_tunnel                   | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR01_TX_tunnel                   | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR_umbrella                      | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR_umbrella_strawb               | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR_STRAWB                        | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR_ext_bis                       | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR8                              | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR7                              | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR6                              | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR5                              | Online     | 0 days 00:00:00     | [----------] |\n",
      "| GHI_EXT_04                        | Online     | 0 days 00:00:00     | [----------] |\n",
      "| GHI_EXT_03                        | Online     | 0 days 00:00:00     | [----------] |\n",
      "| GHI_EXT_02                        | Online     | 0 days 00:00:00     | [----------] |\n",
      "| GHI_EXT                           | Online     | 0 days 00:00:00     | [----------] |\n",
      "| T_RH_ext_bottom                   | Online     | 0 days 00:00:00     | [----------] |\n",
      "| T_RH2                             | Online     | 0 days 00:00:00     | [----------] |\n",
      "| T_RH1                             | Online     | 0 days 00:00:00     | [----------] |\n",
      "| TRH_EXT_top                       | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp_Tulameen_E_NurseryTunnel | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp_Tulameen_W_NurseryTunnel | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp_Vajolet_E_NurseryTunnel  | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp_Vajolet_W_NurseryTunnel  | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp_Tulameen_E_Nursery       | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp_Tulameen_W_Nursery       | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp_Vajolet_E_Nursery        | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp_Vajolet_W_Nursery        | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp02_Joly_Insolagrin05_TX   | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp01_Joly_Insolagrin05_TX   | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp02_Flair_Insolagrin05_TX  | Online     | 0 days 00:00:00     | [----------] |\n",
      "| LeafTemp01_Flair_Insolagrin05_TX  | Online     | 0 days 00:00:00     | [----------] |\n",
      "| leaf_temp_umbrella1               | Online     | 0 days 00:00:00     | [----------] |\n",
      "| leaf_temp_insolagrin_1            | Online     | 0 days 00:00:00     | [----------] |\n",
      "+-----------------------------------+------------+---------------------+--------------+\n",
      "\n",
      "\n",
      "Agroscope Berries\n",
      "+------------------------------------------+------------------+---------------------+--------------+\n",
      "|                                          | Last log         | Time offline (1w)   | % offline    |\n",
      "|------------------------------------------+------------------+---------------------+--------------|\n",
      "| TRH_bottom_Insolagrin03_T60_Portrait_WW  | 2024-02-07 09h06 | 6 days 02:31:42     | [#########-] |\n",
      "| WIND02_EXT                               | 2024-02-07 09h06 | 5 days 19:35:42     | [########--] |\n",
      "| TRH_top_Insolagrin02_T40_Portrait_EW     | 2024-02-07 09h06 | 3 days 04:42:42     | [#####-----] |\n",
      "| TRH_top_Insolagrin01_T60_Portrait_EW     | 2024-02-07 09h06 | 3 days 04:02:42     | [#####-----] |\n",
      "| TRH_top_Insolagrin04_T40_Portrait_WW     | 2024-02-07 09h06 | 3 days 03:48:42     | [#####-----] |\n",
      "| TRH_top_Insolagrin03_T60_Portrait_WW     | 2024-02-07 09h06 | 3 days 03:41:42     | [#####-----] |\n",
      "| TRH_bottom_Insolagrin06_T40_Landscape_EW | 2024-02-07 09h06 | 3 days 03:34:41     | [####------] |\n",
      "| TRH_bottom_Insolagrin02_T40_Portrait_EW  | 2024-02-07 09h06 | 3 days 03:28:42     | [####------] |\n",
      "| TRH_bottom_Insolagrin01_T60_Portrait_EW  | 2024-02-07 09h06 | 3 days 03:25:42     | [####------] |\n",
      "| TRH_bottom_Insolagrin05_T60_Landscape_EW | 2024-02-07 09h06 | 3 days 03:25:41     | [####------] |\n",
      "| SOIL_DOWN_Insolagrin02                   | 2024-02-07 09h06 | 3 days 03:22:42     | [####------] |\n",
      "| SOIL_UP_Insolagrin02                     | 2024-02-07 09h06 | 3 days 03:22:42     | [####------] |\n",
      "| TRH_bottom_Insolagrin04_T40_Portrait_WW  | 2024-02-07 09h06 | 3 days 03:22:42     | [####------] |\n",
      "| RAIN_EXT                                 | 2024-02-07 09h06 | 3 days 03:22:42     | [####------] |\n",
      "| WIND01_EXT                               | 2024-02-07 09h06 | 3 days 03:22:42     | [####------] |\n",
      "| LeafTemp_Z4_int_int                      | 2024-02-07 09h06 | 3 days 03:22:42     | [####------] |\n",
      "| LeafTemp_Z2_side_ext                     | 2024-02-07 09h06 | 3 days 03:22:42     | [####------] |\n",
      "| LeafTemp_Z2_side_int                     | 2024-02-07 09h06 | 3 days 03:22:42     | [####------] |\n",
      "| LeafTemp_Z2_int_int                      | 2024-02-07 09h06 | 3 days 03:22:42     | [####------] |\n",
      "| PAR04_Insolagrin06_T40_Landscape_EW      | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR03_Insolagrin06_T40_Landscape_EW      | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR02_Insolagrin06_T40_Landscape_EW      | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR01_Insolagrin06_T40_Landscape_EW      | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR04_Insolagrin05_T60_Landscape_EW      | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR03_Insolagrin05_T60_Landscape_EW      | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR02_Insolagrin05_T60_Landscape_EW      | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR01_Insolagrin05_T60_Landscape_EW      | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR02_EXT                                | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR01_EXT                                | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR04_Insolagrin04_T40_Portrait_WW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR03_Insolagrin04_T40_Portrait_WW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR02_Insolagrin04_T40_Portrait_WW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR01_Insolagrin04_T40_Portrait_WW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR04_Insolagrin03_T60_Portrait_WW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR03_Insolagrin03_T60_Portrait_WW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR02_Insolagrin03_T60_Portrait_WW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR01_Insolagrin03_T60_Portrait_WW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR04_Insolagrin02_T40_Portrait_EW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR03_Insolagrin02_T40_Portrait_EW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR02_Insolagrin02_T40_Portrait_EW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR01_Insolagrin02_T40_Portrait_EW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR04_Insolagrin01_T60_Portrait_EW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR03_Insolagrin01_T60_Portrait_EW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR02_Insolagrin01_T60_Portrait_EW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| PAR01_Insolagrin01_T60_Portrait_EW       | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| TRH_top_Insolagrin06_T40_Landscape_EW    | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| TRH_top_Insolagrin05_T60_Landscape_EW    | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| TRH_bottom_EXT                           | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "| TRH_top_EXT                              | 2024-02-07 09h06 | 3 days 03:22:41     | [####------] |\n",
      "+------------------------------------------+------------------+---------------------+--------------+\n",
      "\n",
      "\n",
      "Bioschmid\n",
      "+------------------+------------+---------------------+--------------+\n",
      "|                  | Last log   | Time offline (1w)   | % offline    |\n",
      "|------------------+------------+---------------------+--------------|\n",
      "| WIND02_EXT       | Online     | 4 days 06:39:00     | [######----] |\n",
      "| PAR01_EXT        | Online     | 2 days 19:39:00     | [####------] |\n",
      "| PAR02_EXT        | Online     | 0 days 07:51:00     | [----------] |\n",
      "| TRH04_EXT        | Online     | 0 days 02:26:00     | [----------] |\n",
      "| TRH02_EXT        | Online     | 0 days 02:26:00     | [----------] |\n",
      "| TRH02_kontroll   | Online     | 0 days 00:56:43     | [----------] |\n",
      "| TRH01_EXT        | Online     | 0 days 00:55:00     | [----------] |\n",
      "| TRH01_INSOLAGRIN | Online     | 0 days 00:39:00     | [----------] |\n",
      "| TRH01_agriverti  | Online     | 0 days 00:31:00     | [----------] |\n",
      "| TRH02_INSOLAGRIN | Online     | 0 days 00:30:00     | [----------] |\n",
      "| PAR04_INSOLAGRIN | Online     | 0 days 00:24:00     | [----------] |\n",
      "| PAR03_INSOLAGRIN | Online     | 0 days 00:24:00     | [----------] |\n",
      "| PAR02_INSOLAGRIN | Online     | 0 days 00:24:00     | [----------] |\n",
      "| PAR01_INSOLAGRIN | Online     | 0 days 00:24:00     | [----------] |\n",
      "| GHI_01_EXT       | Online     | 0 days 00:24:00     | [----------] |\n",
      "| RAIN01_EXT       | Online     | 0 days 00:24:00     | [----------] |\n",
      "| WIND01_EXT       | Online     | 0 days 00:24:00     | [----------] |\n",
      "| TRH02_agriverti  | Online     | 0 days 00:12:00     | [----------] |\n",
      "| TRH03_EXT        | Online     | 0 days 00:06:00     | [----------] |\n",
      "| PAR03_agriverti  | Online     | 0 days 00:05:00     | [----------] |\n",
      "| PAR02_agriverti  | Online     | 0 days 00:05:00     | [----------] |\n",
      "| PAR01_agriverti  | Online     | 0 days 00:05:00     | [----------] |\n",
      "| TRH01_kontroll   | Online     | 0 days 00:02:43     | [----------] |\n",
      "| PAR03_kontroll   | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR02_kontroll   | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR01_kontroll   | Online     | 0 days 00:00:00     | [----------] |\n",
      "+------------------+------------+---------------------+--------------+\n",
      "\n",
      "\n",
      "Etchelecu\n",
      "+-----------------------+------------+---------------------+--------------+\n",
      "|                       | Last log   | Time offline (1w)   | % offline    |\n",
      "|-----------------------+------------+---------------------+--------------|\n",
      "| TRH_bottom_Insolagrin | Online     | 0 days 01:04:00     | [----------] |\n",
      "| TRH_bottom_EXT        | Online     | 0 days 00:09:00     | [----------] |\n",
      "| TRH_bottom_control    | Online     | 0 days 00:06:00     | [----------] |\n",
      "| TRH_top_Insolagrin    | Online     | 0 days 00:03:00     | [----------] |\n",
      "| PAR02_control         | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR01_control         | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR02_EXT             | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR01_EXT             | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR04_Insolagrin      | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR03_Insolagrin      | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR02_Insolagrin      | Online     | 0 days 00:00:00     | [----------] |\n",
      "| PAR01_Insolagrin      | Online     | 0 days 00:00:00     | [----------] |\n",
      "| GHI_01_EXT            | Online     | 0 days 00:00:00     | [----------] |\n",
      "| TRH_top_control       | Online     | 0 days 00:00:00     | [----------] |\n",
      "| TRH_top_EXT           | Online     | 0 days 00:00:00     | [----------] |\n",
      "| RAIN_EXT              | Online     | 0 days 00:00:00     | [----------] |\n",
      "| WIND02_EXT            | Online     | 0 days 00:00:00     | [----------] |\n",
      "| WIND01_EXT            | Online     | 0 days 00:00:00     | [----------] |\n",
      "+-----------------------+------------+---------------------+--------------+\n",
      "\n",
      "\n",
      "Qair\n",
      "+------------+---------------------+-------------+\n",
      "| Last log   | Time offline (1w)   | % offline   |\n",
      "|------------+---------------------+-------------|\n",
      "+------------+---------------------+-------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the last logs for each installation and sensor\n",
    "logs_joined_unique = last_logs(dict_instal_json, list_sensor, api)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataViz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
